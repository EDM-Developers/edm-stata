{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Empirical Dynamic Modeling Stata Package","text":""},{"location":"#package-description","title":"Package Description","text":"<p>Empirical Dynamic Modeling (EDM) is a way to perform causal analysis on time series data.  The <code>edm</code> Stata package implements a series of EDM tools, including the convergent cross-mapping algorithm. </p> <p>Key features of the package:</p> <ul> <li>powered by a fast multi-threaded C++ backend,</li> <li>able to process panel data, a.k.a. multispatial EDM,</li> <li>able to handle missing data using new <code>dt</code> algorithms or by dropping points,</li> <li>factor variables can be added to the analysis,</li> <li>multiple distance functions available (Euclidean, Mean Absolute Error, Wasserstein),</li> <li>GPU acceleration available.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>To install the stable version directly through Stata:</p> <pre><code>ssc install edm, replace\n</code></pre> <p>To install the latest development version, first install the stable version from SSC then inside Stata run:</p> <pre><code>edm update, development replace\n</code></pre> <p>The source code for the package is available on Github.</p>"},{"location":"#r-python-packages","title":"R &amp; Python packages","text":"<p>We are currently creating the fastEDM R package and the fastEDM Python package which are direct ports of this Stata package to R &amp; Python. As all the packages share the same underlying C++ code, their behaviour will be identical.</p>"},{"location":"#other-resources","title":"Other Resources","text":"<p>This site serves as the primary source of documentation for the package, though there is also:</p> <ul> <li>our Stata Journal paper which explains the package and the overall causal framework, and</li> <li>Jinjing's QMNET seminar on the package, the recording is on YouTube and the slides are here.</li> <li>Patrick's short presentation on the EDM packages to the Time Series and Forecasting Symposium 2022:</li> </ul> <p> <p></p>"},{"location":"#authors","title":"Authors","text":"<ul> <li>Jinjing Li (author),</li> <li>Michael Zyphur (author),</li> <li>Patrick Laub (author, maintainer),</li> <li>Edoardo Tescari (contributor),</li> <li>Simon Mutch (contributor),</li> <li>George Sugihara (originator)</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>Jinjing Li, Michael J. Zyphur, George Sugihara, Patrick J. Laub (2021), Beyond Linearity, Stability, and Equilibrium: The edm Package for Empirical Dynamic Modeling and Convergent Cross Mapping in Stata, Stata Journal, 21(1), pp. 220-258</p> <pre><code>@article{edm-stata,\n  title={Beyond linearity, stability, and equilibrium: The edm package for empirical dynamic modeling and convergent cross-mapping in {S}tata},\n  author={Li, Jinjing and Zyphur, Michael J and Sugihara, George and Laub, Patrick J},\n  journal={The Stata Journal},\n  volume={21},\n  number={1},\n  pages={220--258},\n  year={2021},\n}\n</code></pre>"},{"location":"adding-to-manifold/","title":"Adding extra variables to the manifold","text":"<p>It can be advantageous to combine data from multiple sources into a single EDM analysis.</p> <p>The <code>extra</code> command will incorporate additional pieces of data into the manifold. As an example, consider the Stata command</p> <p><code>edm explore a, extra(b)</code></p> <p>Choose the number of observations</p> <p></p> <p>Choose a value for \\(E\\)</p> <p></p> <p>Choose a value for \\(\\tau\\)</p> <p></p> <p>The time-delayed embedding of the \\(a\\) time series with the given \\(E\\) and \\(\\tau\\) is the manifold:</p> <p></p> <p>However, with the <code>extra(b)</code> option, we use the time-delayed embedding with the extra time series included like:</p> <p></p> <p>After extra variables are added, the manifold \\(M_{a,b}\\) no longer has \\(E\\) columns. In these cases, we make a distinction between \\(E\\) which selects the number of lags for each time series, and the actual \\(E\\) which is size of each point (i.e. the number of columns).</p> <p>By default just one \\(b\\) observation is added to each point in the manifold. However, this will only allow us to capture to the contemporaneous effect of \\(b\\) on the system.</p> <p>If instead we want to add the last \\(E\\) lags of \\(b\\) (just as we do with the \\(a\\) time series series), then the command should be altered slightly to</p> <p><code>edm explore a, extra(b(e))</code></p> <p>and then the manifold will be:</p> <p></p> <p>More than one <code>extra</code> variable can be added, and any combinations of \\(E\\)-varying and non-\\(E\\)-varying extras are permitted.</p> <p>Note</p> <p>If some extras are lagged extra variables (i.e. \\(E\\)-varying extras) and they are specified after some unlagged extras, then the package will reorder them so that all the lagged extras are first.</p>"},{"location":"api/","title":"Stata package help file","text":"<p>Note</p> <p>This page simply reproduces, for convenience, the output of typing <code>help edm</code> into the Stata console.</p> <p>The command edm implements a series of tools that can be used for empirical dynamic modeling in Stata. The core algorithm is written in C++ (with a Mata backup) to achieve reasonable execution speed. The command keyword is edm, and should be immediately followed by a subcommand such as explore or xmap. A dataset must be declared as time-series or panel data by the tsset or xtset command prior to using the edm command, and time-series operators including l., f., d., and s. can be used (the last for seasonal differencing).</p>"},{"location":"api/#syntax","title":"Syntax","text":"<p>The explore subcommand follows the syntax below and supports one variable for exploration using simplex projection or S-mapping.</p> <pre><code>edm explore variable [if exp], [e(numlist ascending &gt;=2)]\n[tau(integer 1)] [theta(numlist ascending)] [k(integer 0)] [ALGorithm(string)] [REPlicate(integer 0)]\n[seed(integer 0)] [full] [RANDomize] [PREDICTionsave(name)] [COPREDICTionsave(name)] [copredictvar(string)]\n[CROSSfold(integer 0)] [CI(integer 0)] [EXTRAembed(string)] [ALLOWMISSing] [MISSINGdistance(real 0)]\n[dt] [reldt] [DTWeight(real 0)] [DTSave(name)] [DETails] [reportrawe] [strict] [Predictionhorizon(string)]\n[dot(integer 1)] [mata] [gpu] [nthreads(integer 0)] [savemanifold(name)] [idw(real 0)]\n[lowmemory]\n</code></pre> <p>The second subcommand xmap performs convergent cross-mapping (CCM). The subcommand follows the syntax below and requires two variables to follow immediately after xmap. It shares many of the same options with the explore subcommand although there are some differences given the different purpose of the analysis.</p> <pre><code>edm xmap variables [if exp], [e(integer 2)] [tau(integer 1)] [theta(real 1)]\n[Library(numlist)] [RANDomize] [k(integer 0)] [ALGorithm(string)] [REPlicate(integer 0)] [strict]\n[DIrection(string)] [seed(integer 0)] [PREDICTionsave(name)] [COPREDICTionsave(name)] [copredictvar(string)]\n[CI(integer 0)] [EXTRAembed(string)] [ALLOWMISSing] [MISSINGdistance(real 0)] [dt] [reldt]\n[DTWeight(real 0)] [DTSave(name)] [oneway] [DETails] [SAVEsmap(string)] [Predictionhorizon(string)]\n[dot(integer 1)] [mata] [gpu] [nthreads(integer 0)] [savemanifold(name)] [idw(real 0)]\n[lowmemory]\n</code></pre> <p>The third subcommand update updates the plugin to its latest version</p> <pre><code>edm update, [develop] [replace]\n</code></pre> <p>The fourth subcommand version displays the current version number</p> <pre><code>edm version\n</code></pre>"},{"location":"api/#options","title":"Options","text":""},{"location":"api/#options-for-explore-and-xmap-subcommands","title":"Options for explore and xmap subcommands","text":"<p>e(numlist ascending): This option specifies the number of dimensions E used for the main variable in the manifold reconstruction. If a list of numbers is provided, the command will compute results for all numbers specified. The xmap subcommand only supports a single integer as the option whereas the explore subcommand supports the option as a numlist. The default value for E is 2, but in theory E can range from 2 to almost half of the total sample size. The actual E used in the estimation may be different if additional variables are incorporated. A error message is provided if the specified value is out of range. Missing data will limit the maximum E under the default deletion method.</p> <p>tau(integer): The tau (or \u03c4) option allows researchers to specify the \u2018time delay\u2019, which essentially sorts the data by the multiple \u03c4. This is done by specifying lagged embeddings that take the form: t,t-1\u03c4,\u2026,t-(E-1)\u03c4, where the default is tau(1) (i.e., typical lags). However, if tau(2) is set then every-other t is used to reconstruct the attractor and make predictions\u2014this does not halve the observed sample size because both odd and even t would be used to construct the set of embedding vectors for analysis. This option is helpful when data are oversampled (i.e., spaced too closely in time) and therefore very little new information about a dynamic system is added at each occasion. However, the tau() setting is also useful if different dynamics occur at different times scales, and can be chosen to reflect a researcher\u2019s theory-driven interest in a specific time-scale (e.g., daily instead of hourly). Researchers can evaluate whether \u03c4&gt;1 is required by checking for large autocorrelations in the observed data (e.g., using Stata\u2019s corrgram function). Of course, such a linear measure of association may not work well in nonlinear systems and thus researchers can also check performance by examining \u03c1 and MAE at different values of \u03c4.</p> <p>theta(numlist ascending): Theta (or \u03b8) is the distance weighting parameter for the local neighbours in the manifold. It is used to detect the nonlinearity of the system in the explore subcommand for S-mapping. Of course, as noted above, for simplex projection and CCM a weight of <code>theta(1)</code> is applied to neighbours based on their distance, which is reflected in the fact that the default value of \u03b8 is 1. However, this can be altered even for simplex projection or CCM (two cases that we do not cover here). Particularly, values for S-mapping to test for improved predictions as they become more local may include the following command: <code>theta(0 .00001 .0001 .001 .005 .01 .05 .1 .5 1 1.5 2 3 4 6 8 10)</code>.</p> <p>k(integer): This option specifies the number of neighbours used for prediction. When set to 1, only the nearest neighbour is used, but as k increases the next-closest nearest neighbours are included for making predictions. In the case that k is set 0, the number of neighbours used is calculated automatically (typically as k = E + 1 to form a simplex around a target), which is the default value. When k &lt; 0 (e.g., <code>k(-1)</code>), all possible points in the prediction set are used (i.e., all points in the library are used to reconstruct the manifold and predict target vectors). This latter setting is useful and typically recommended for S-mapping because it allows all points in the library to be used for predictions with the weightings in theta. However, with large datasets this may be computationally burdensome and therefore <code>k(100)</code> or perhaps <code>k(500)</code> may be preferred if T or NT is large.</p> <p>ALGorithm(string): This option specifies the algorithm used for prediction. If not specified, simplex projection (a locally weighted average) is used. Valid options include simplex and smap, the latter of which is a sequential locally weighted global linear mapping (or S-map as noted previously). In the case of the xmap subcommand where two variables predict each other, the algorithm(smap) invokes something analogous to a distributed lag model with E + 1 predictors (including a constant term c) and, thus, E + 1 locally-weighted coefficients for each predicted observation/target vector\u2014because each predicted observation has its own type of regression done with k neighbours as rows and E + 1 coefficients as columns. As noted below, in this case special options are available to save these coefficients for post-processing but, again, it is not actually a regression model and instead should be seen as a manifold.</p> <p>RANDomize: When splitting the observations into library and prediction sets, by default the oldest observations go into the library set and the newest observations to the prediction set. Though if the randomize option is specified, the data is allocated into the two sets in a random fashion. If the replicate option is specified, then this randomization is enabled automatically.</p> <p>REPlicate(integer): The explore subcommand uses a random 50/50 split for simplex projection and S-maps, whereas the xmap subcommand selects the observations randomly for library construction if the size of the library L is smaller than the size of all available observations. In these cases, results may be different in each run because the embedding vectors (i.e., the E-dimensional points) used to reconstruct a manifold are chosen at random. The replicate option takes advantages of this to allow repeating the randomization process and calculating results each time. This is akin to a nonparametric bootstrap without replacement, and is commonly used for inference using confidence intervals in EDM (Tsonis et al., 2015; van Nes et al., 2015; Ye et al., 2015b). When replicate is specified, such as replicate(50), mean values and the standard deviations of the results are reported across the 50 runs by default. As we note below, it is possible to save all estimates for post-processing using typical Stata commands such as svmat, allowing the graphing of results or finding percentile-based with the pctile command.</p> <p>PREDICTionsave(variable): This option allows you to save the edm predictions as a variable, which could be useful for plotting and diagnosis.</p> <p>COPREDICTionsave(variable): This option allows you to save the copredictions as a variable. You must specify the copredictvar(variables) options for this to work.</p> <p>copredictvar(variable): This option specifies the variable used for coprediction. A second prediction is run for each configuration of E, library, etc., using the same library set but with a prediction set built from the lagged embedding of this variable.</p> <p>Predictionhorizon(integer): This option adjusts the default number of observations ahead which we predict. By default, the explore mode predict \u03c4 observations ahead and the xmap mode uses p(0). This parameter can be negative.</p> <p>DETails: By default, only mean values and standard deviations are reported when the replicate option is specified. The details option overrides this behaviour by providing results for each individual run. Irrespective of using this option, all results can be saved for post-processing.</p> <p>CI(integer): When used with replicate() or crossfold(), this option reports the confidence interval for the mean of the estimates (MAE and/or \u03c1), as well as the percentiles of their distribution. The first row of output labelled \u201cEst. mean CI\u201d reports the estimated confidence interval of the mean \u03c1, assuming that \u03c1 has a normal distribution\u2014estimated as the corrected sample standard deviation (with N-1 in the denominator) divided by the squared root of the number of replications. The reported range can be used to compare mean \u03c1 across different (hyper) parameter values (e.g., different E, \u03b8, or L) using the same datasets as if the sample was the entire population (such that uncertainty is reduced to 0 when the number of replications \u2192\u221e). These intervals can be used to test which (hyper) parameter values best describe a sample, as might be typically used when using crossfold validation methods. The row labelled with \u201cPc (Est.)\u201d follows the same normality assumption and reports the estimated percentile values based on the corrected sample standard deviation of the replicated estimates. The row labelled \u201cPc (Obs.)\u201d reports the actual observed percentile values from the replicated estimates. In both of these latter cases the percentile values offer alternative metrics for comparisons across distributions, which would be more useful for testing typical hypotheses about population differences in estimates across different (hyper) parameter values (e.g., different E, \u03b8, or L), such as testing whether a dynamical system appears to be nonlinear in a population (i.e., testing whether \u03c1 is maximized when \u03b8 &gt; 0). The number specified within the ci() bracket determines the confidence level and the locations of the percentile cut-offs. For example, ci(90) instructs edm to return 90% CI as well as the cut-off values for the 5th and 95th percentile values\u2014because \u03c1 and MAE values cannot or are not expected to take on negative values, we typically prefer one-tailed hypothesis tests and therefore would use ci(90) to get a one-tailed 95% interval. These estimated ranges are also included in the e() return list as a series of scalars with names starting with \u201cub\u201d for upper bound and \u201clb\u201d for lower bound values of the CIs. These return values can be used for further post-processing.</p> <p>seed(integer): This option specifies the seed used for the random number. In some special cases users may wish to use this in order to keep library and prediction sets the same across simplex projection and S-mapping with a single variable, or across multiple CCM runs with different variables. Note: if <code>set rng</code> has been used to change Stata's random number generation algorithm, then edm will temporarily change it the default 64 bit Mersenne twister internally.</p> <p>strict: When this option is specified, the computation will fail if the requested number of neighboring observations is not present. By default, if not all of the request neighbors are available, the computation will just continue using as many as possible.</p> <p>ALLOWMISSing This option allows observations with missing values to be used in the manifold. Vectors with at least one non-missing values will be used in the manifold construction. Distance computations are adapted to allow missing values when this option is specified.</p> <p>MISSINGdistance(real): This option allows users to specify the assumed distance between missing values and any values (including missing) when estimating the Euclidean distance of the vector. This enables computations with missing values. The option implies allowmissing. By default, the distance is set to the expected distance of two random draws in a normal distribution, which equals to 2/sqrt(pi) * standard deviation of the mapping variable.</p> <p>EXTRAembed(variables): This option allows incorporating additional variables into the embedding (multivariate embedding), e.g. <code>extra(z l.z)</code>. Time series lists are unabbreviated here, e.g. <code>extra(L(1/3).z)</code> will be equivalent to <code>extra(L1.z L2.z L3.z)</code>. Normally, lagged versions of the extra variables are not included in the embedding, however the syntax <code>extra(z(e))</code> includes E lags of z in the embedding.</p> <p>dt: This option allows automatic inclusion of the timestamp differencing in the embedding. There will be E dt variables included for an embedding with E dimensions. By default, the weights used for these additional variables equal to the standard deviation of the main mapping variable divided by the standard deviation of the time difference. This can be overridden by the <code>dtweight()</code> option. <code>dt</code> option will be ignored when running with data with no sampling variation in the time lags. The first dt variable embeds the time of the between the most recent observation and the time of the corresponding target/predictand.</p> <p>reldt: This option, to be read as 'relative dt', is like the 'dt' option above in that it includes E extra variables for an embedding with E dimensions. However the timestamp differences added are not the time between the corresponding observations, but the time of the target/predictand minus the time of the lagged observations.</p> <p>DTWeight(real): This option specifies the weight used for the timestamp differencing variable.</p> <p>DTSave(variable): This option allows users to save the internally generated timestamp differencing variable.</p> <p>nthreads(integer): The number of threads the C++ plugin will use for parallel computations. The default is the number of cores available on the host computer.</p> <p>idw(real): This parameter is used when xtset indicates that the current dataset is panel data. Then, idw specifies a penalty that is added to the distances between points in the manifold which correspond to observations from different panels. By default idw is 0, so the data from all panels is mixed together and treatly equally. If idw(-1) is set (or any other negative value), then the weight is treated as 'infinity', so neighbours will never be selected which cross the boundaries between panels. Setting idw(-1) with k(-1) means we may use a different number of neighbors for different predictions (i.e. if the panels are unbalanced).</p> <p>lowmemory: It is possible that RAM may be depleted while running edm on large datasets with large values of E. The lowmemory flag directs the plugin to try to save as much space as possible by more efficiently using memory, though for small datasets this will likely slow down the computations by a small but noticeable amount.</p> <p>Besides the shared parameters, edm explore supports the following extra options:</p> <p>CROSSfold(integer): This option asks the program to run a cross-fold validation of the predicted variables. crossfold(5) indicates a 5-fold cross validation. Note that this cannot be used together with replicate. This option is only available with the <code>explore</code> subcommand.</p> <p>full: When this option is specified, the explore command will use all possible observations in the manifold construction instead of the default 50/50 split. This is effectively the same as leave-one-out cross-validation as the observation itself is not used for the prediction.</p> <p>reportrawe: By default, the program reports the actual E used in the manifold. With this option, the program will only report the number of dimensions constructed from the main variable.</p> <p>Library(numlist ascending): This option specifies the total library size L used for the manifold reconstruction. Varying the library size is used to estimate the convergence property of the cross-mapping, with a minimum value Lmin = E + 2 and the maximum equal to the total number of observations minus sufficient lags (e.g., in the time-series case without missing data this is Lmax = T + 1 \u2013 E). An error message is given if the L value is beyond the allowed range. To assess the rate of convergence (i.e., the rate at which \u03c1 increases as L grows), the full range of library sizes at small values of L can be used, such as if E = 2 and T = 100, with the setting then perhaps being <code>library(4(1)25 30(5)50 54(15)99)</code>. This option is only available with the <code>xmap</code> subcommand.</p> <p>SAVEsmap(string): This option allows smap coefficients to be stored in variables with a specified prefix. For example, specifying <code>edm xmap x y, algorithm(smap) savesmap(beta) k(-1)</code> will create a set of new variables such as beta1_b0_rep1. The string prefix (e.g., 'beta') must not be shared with any variables in the dataset, and the option is only valid if the algorithm(smap) is specified. In terms of the saved variables such as beta1_b0_rep1, the first number immediately after the prefix \u2018beta\u2019 is 1 or 2 and indicates which of the two listed variables is treated as the dependent variable in the cross-mapping (i.e., the direction of the mapping). For the <code>edm xmap x y</code> case, variables starting with <code>beta1_</code> contain coefficients derived from the manifold M_X created using the lags of the first variable \u2018x\u2019 to predict Y, or Y|M_X. This set of variables therefore store the coefficients related to \u2018x\u2019 as an outcome rather than a predictor in CCM. Keep in mind that any Y\u2192X effect associated with the <code>beta1_</code> prefix is shown as Y|M_X, because the outcome is used to cross-map the predictor, and thus the reported coefficients will be scaled in the opposite direction of a typical regression (because in CCM the outcome variable predicts the cause). To get more familiar regression coefficients (which will be locally weighted), variables starting with beta2_ store the coefficients estimated in the other direction, where the second listed variable \u2018y\u2019 is used for the manifold reconstruction M_Y for the mapping X|M_Y in the \u201cedm xmap x y\u201d case, testing the opposite X\u2192Y effect in CCM, but with reported S-map coefficients that map to a Y\u2192X regression. We appreciate that this may be unintuitive, but because CCM causation is tested by predicting the causal variable with the outcome, to get more familiar regression coefficients requires reversing CCM\u2019s causal direction to a more typical predictor\u2192outcome regression logic. This can be clarified by reverting to the conditional notation such as X|M_Y, which in CCM implies a left-to-right X\u2192Y effect, but for the S-map coefficients will be scaled as a locally-weighted regression in the opposite direction Y\u2192X. Moving on, following the 1 and 2 is the letter b and a number. The numerical labeling scheme generally follows the order of the lag for the main variable and then the order of the extra variables introduced in the case of multivariate embedding. b0 is a special case which records the coefficient of the constant term in the regression. The final term rep1 indicates the coefficients are from the first round of replication (if the replicate() option is not used then there is only one). Finally, the coefficients are saved to match the observation t in the dataset that is being predicted, which allows plotting each of the E estimated coefficients against time and/or the values of the variable being predicted. The variables are also automatically labelled for clarity. This option is only available with the <code>xmap</code> subcommand.</p> <p>DIrection(string): This option allows users to control whether the cross mapping is calculated bidirectionally or unidirectionally, the latter of which reduces computation times if bidirectional mappings are not required. Valid options include \u201coneway\u201d and \u201cboth\u201d, the latter of which is the default and computes both possible cross-mappings. When oneway is chosen, the first variable listed after the xmap subcommand is treated as the potential dependent variable following the conventions in the regression syntax of Stata such as the\u2018reg\u2019 command, so <code>edm xmap x y, direction(oneway)</code> produces the cross-mapping Y|M_X, which pertains to a Y\u2192X effect. This is consistent with the beta1_ coefficients from the previous savesmap(beta) option. On this point, the direction(oneway) option may be especially useful when an initial \u201cedm xmap x y\u201dprocedure shows convergence only for a cross-mapping Y|M_X, which pertains to a Y\u2192X effect. To save time with large datasets, any follow-up analyses with the algorithm(smap) option can then be conducted with <code>edm xmap x y, algorithm(smap) savesmap(beta) direction(oneway)</code>.  To make this easier there is also a simplified oneway option that implies direction(oneway). This option is only available with the <code>xmap</code> subcommand.</p> <p>oneway: This option is equivalent to <code>direction(oneway)</code></p>"},{"location":"api/#options-for-update-subcommand","title":"Options for update subcommand","text":"<p>The update subcommand supports the following options:</p> <p>develop: This option updates the command to its latest development version. The development version usually contains more features but may be less tested compared with the older version distributed on SSC.</p> <p>replace: This option specifies whether you allow the update to override your local ado files.</p>"},{"location":"api/#examples","title":"Examples","text":"<p>Chicago crime dataset example (included in the auxiliary file)</p> <pre><code>use chicago,clear\n</code></pre> <pre><code>edm explore temp, e(2/30)\n</code></pre> <pre><code>edm xmap temp crime\n</code></pre> <pre><code>edm xmap temp crime, alg(smap) savesmap(beta) e(6) k(-1)\n</code></pre>"},{"location":"api/#updates","title":"Updates","text":"<p>To install the stable version or upgrade directly through Stata:</p> <pre><code>edm update, replace\n</code></pre> <p>To install the development version directly through Stata:</p> <pre><code>edm update, develop replace\n</code></pre>"},{"location":"automated-workflows/","title":"Automated workflow scripts","text":"<p>Bug</p> <pre><code>There are currently known bugs in the following do scripts.\nUntil we can fix them, we suggest that you either: i) directly use the `edm` commands as per the examples on this site, ii) use the scripts as an inspiration for your do files, or iii) help us track down the bugs with a pull request to [https://github.com/EDM-Developers/edm-stata/](https://github.com/EDM-Developers/edm-stata/), or iv) try out the `easy_edm` commands in the `fastEDM` packages.\n</code></pre> <p>Linked below are some Stata scripts using <code>edm</code> which automate some common analyses. They provide some early stage prototypes allowing automated Simplex Projections, S-maps, Coprediction, and Convergent Cross Mapping as well as relevant hypothesis tests:</p> <ul> <li>edm Plugin - Automated time-series analysis (N=1 case)</li> <li>edm Plugin - Automated multi-spatial analysis (N&gt;1 panel data case)</li> <li>edm Plugin - Automated multiple-edm analysis (N&gt;1 panel data case)</li> </ul>"},{"location":"coprediction/","title":"Coprediction","text":""},{"location":"coprediction/#what-does-copredict-do-in-explore-mode","title":"What does <code>copredict</code> do in explore mode?","text":"<p>Imagine that we use the command:</p> <pre><code>edm explore a, copredictvar(c) copredict(out)\n</code></pre> <p>This will first do a normal</p> <pre><code>edm explore a\n</code></pre> <p>operation, then it will perform a second set of copredictions. This brings in a second time series \\(c\\), and specifies that the predictions made in copredict mode should be stored in the Stata variable named <code>out</code>.</p> <p>For the following, let's first set the general manifold parameters.</p> <p>Choose the number of observations</p> <p></p> <p>Choose a value for \\(E\\)</p> <p></p> <p>Choose a value for \\(\\tau\\)</p> <p></p> <p>In coprediction mode, the training set will include the entirety of the \\(M_a\\) manifold and its projections:</p> <p></p> <p>In copredict mode the most significant difference is that we change \\(\\mathscr{P}\\) to be the \\(M_c\\) manifold for the \\(c\\) time series and \\(\\mathbf{y}_{\\mathscr{P}}\\) to:</p> <p></p> <p>The rest of the simplex procedure is the same as before:</p> \\[     \\begin{aligned}         \\underbrace{ \\text{For target } y_i }_{ \\text{Based on } c }         &amp; \\underset{\\small \\text{Get predictee}}{\\Rightarrow}         \\underbrace{ \\mathbf{x}_{i} }_{ \\text{Based on } c}         \\underset{\\small \\text{Find neighbours in } \\mathscr{L}}{\\Rightarrow}         \\mathcal{NN}_k(i) \\\\         &amp;\\,\\,\\,\\,         \\underset{\\small \\text{Extracts}}{\\Rightarrow}         \\{ y_{[j]}\\}_{j \\in \\mathcal{NN}_k(i)}         \\underset{\\small \\text{Make prediction}}{\\Rightarrow}         \\hat{y}_i     \\end{aligned} \\]"},{"location":"coprediction/#what-does-copredict-do-in-xmap-mode","title":"What does <code>copredict</code> do in xmap mode?","text":"<p>Imagine that we use the command:</p> <pre><code>edm xmap a b, oneway copredictvar(c) copredict(out)\n</code></pre> <p>Now we combine three different time series to create the predictions in the <code>out</code> Stata variable.</p> <p>In this case, the training set contains all the points in \\(M_a\\):</p> <p></p> <p>The main change in coprediction is the prediction set and the targets are based on the new \\(c\\) time series:</p> <p></p> <p>Finally, the simplex prediction steps are the same, with:</p> \\[  \\underbrace{ \\text{For target }y_i }_{\\text{Based on } c}     \\underset{\\small \\text{Get predictee}}{\\Rightarrow}     \\underbrace{ \\mathbf{x}_i }_{ \\text{Based on } c }     \\underset{\\small \\text{Find neighbours in}}{\\Rightarrow}     \\underbrace{ \\mathscr{L} }_{\\text{Based on } a}     \\underset{\\small \\text{Matches}}{\\Rightarrow}     \\underbrace{ \\{ y_{[j]}\\}_{j \\in \\mathcal{NN}_k(i)} }_{\\text{Based on } b}     \\underset{\\small \\text{Make prediction}}{\\Rightarrow}     \\underbrace{ \\hat{y}_i }_{\\text{Based on } b} \\]"},{"location":"explore/","title":"The <code>explore</code> subcommand","text":"<p>The <code>explore</code> subcommand focuses on taking a time series and trying to forecast the values it takes in the near future. In the grand scheme of empirical dynamic modelling, it is usually just the first step one would use in order to find the appropriate hyperparameters to choose before running a convergent cross mapping analysis (using the <code>xmap</code> command).</p>"},{"location":"explore/#setup","title":"Setup","text":"<p>Before describing the mechanics of the <code>explore</code> subcommand, we first need to take the given time series data, convert it into time-delayed reconconstructions, and define some notation.</p>"},{"location":"explore/#create-the-time-delayed-embedding","title":"Create the time-delayed embedding","text":"<p>Given the time series \\(a_t\\), we create the time-delayed embedding.</p> <p>Choose the number of observations</p> <p></p> <p>Choose a value for \\(E\\)</p> <p></p> <p>Choose a value for \\(\\tau\\)</p> <p></p> <p>The time-delayed embedding of the \\(a\\) time series with the selected \\(E\\) and \\(\\tau\\) is the manifold:</p> <p></p>"},{"location":"explore/#split-the-data","title":"Split the data","text":"<p>The manifold \\(M_a\\) is then split into two parts, called the library set denoted \\(\\mathscr{L}\\) and the prediction set denoted \\(\\mathscr{P}\\).</p> <p>By default, we take the points of the \\(M_a\\) manifold and assign the half of them to \\(\\mathscr{L}\\) and the other half to \\(\\mathscr{P}\\).</p> <p>In this case, the library set is</p> <p></p> <p>and the prediction set is</p> <p></p> <p>It will help to introduce a notation to refer to a specific point in these sets based on its row number. E.g. in the example above, the first point in the library set is denoted</p> <p></p> <p>while the first point in the prediction set is denoted</p> <p></p> <p>More generally \\(\\mathbf{x}_{[i]}\\) refers to the \\(i\\)th point in \\(\\mathscr{L}\\) while \\(\\mathbf{x}_{j}\\) refers to the \\(j\\)th point in \\(\\mathscr{P}\\).</p> <p>Note</p> <p>In the default case, the same point doesn't appear in both \\(\\mathscr{L}\\) and \\(\\mathscr{P}\\), though given other options then the same point may appear in both sets (though during the prediction process, we would ignore the duplicate point in \\(\\mathscr{L}\\) for that prediction task). This is one good reason why we don't simply call these sets the 'train' and 'test' sets.</p>"},{"location":"explore/#look-into-the-future","title":"Look into the future","text":"<p>Each point on the manifold refers to a small trajectory of a time series, and for each point we look \\(p\\) observations into the future of the time series.</p> <p>Choose a value for \\(p\\)</p> <p></p> <p>So if we take the first point of the prediction set \\(\\mathbf{x}_{1}\\) and say that \\(y_1\\) is the value it takes \\(p\\) observations in the future, we get:</p> <p></p> <p></p> <p>This \\(p\\) may be thought of as the prediction horizon, and in <code>explore</code> mode is defaults to \\(\\tau\\) and in <code>xmap</code> mode it defaults to 0.</p> <p>Our \\(p\\) versus the \\(T_p\\) which is common in the literature</p> <p>In the literature, instead of measuring the number of observations \\(p\\) ahead, authors normally use the value \\(T_p\\) to denote the amount of time this corresponds to. When data is regularly sampled (e.g. \\(t_i = i\\)) then there is no difference (e.g. \\(T_p = p\\)), however for irregularly sampled data the actual time difference may be different for each prediction.</p> <p>In the training set, this means each point of \\(\\mathscr{L}\\) matches the corresponding value in \\(\\mathbf{y}_{\\mathscr{L}}\\):</p> <p></p> <p>Similarly, for the prediction set:</p> <p></p>"},{"location":"explore/#what-does-edm-explore-a-do","title":"What does <code>edm explore a</code> do?","text":"<p>When running <code>edm explore a</code>, we pretend that we don't know the values in \\(\\mathbf{y}_{\\mathscr{P}}\\) and that we want to predict them given we know \\(\\mathscr{L}\\) and \\(\\mathbf{y}_{\\mathscr{L}}\\).</p> <p>The first prediction is to try to find the value of \\(y_1\\) given the corresponding point \\(\\mathbf{x}_1\\):</p> <p></p> <p>We will use \\(\\mathbf{x}_1\\) to predict \\(y_1\\), so the \\(\\mathbf{x}_1\\) values can be viewed as covariates and \\(y_1\\) as the target of the prediction.</p> <p>There are two main algorithms used to make these predctions: the simplex algorithm, and the S-map method. For those familiar with traditional statistical learning techniques, it may be helpful to think of the simplex algorithm as being a variation on the \\(k\\)-nearest neighbours method, and S-map being a modification of the multivariate linear regression method or equivalently the autoregressive time series model.</p>"},{"location":"explore/#using-the-simplex-algorithm","title":"Using the simplex algorithm","text":"<p>If we have chosen the <code>algorithm</code> to be the simplex algorithm, then we start by finding the \\(k\\) points in \\(\\mathscr{L}\\) which are closest to the given \\(\\mathbf{x}_1\\).</p> <p>Let's say that we chose \\(k=2\\) and also pretend that the two most similar points to \\(\\mathbf{x}_1\\) are \\(\\mathbf{x}_{[3]}\\) and \\(\\mathbf{x}_{[5]}\\). We will choose the notation </p> \\[     \\mathcal{NN}_k(\\mathbf{x}_{1}) = \\{ 3, 5 \\} \\] <p>to describe the set of the indices for the \\(k\\) nearest neighbours of \\(\\mathbf{x}_{1}\\).</p> <p>In this process, we also store the distances (by default, using Euclidean distance) between \\(\\mathbf{x}_{1}\\) and its nearest neighbours, giving special attention to the smallest distance \\(d_{\\text{min}} &gt; 0\\).</p> <p>Then, we predict that</p> \\[     \\hat{y}_1 := w_1 \\times y_{[3]} + w_2 \\times y_{[5]} \\] <p>where \\(w_1\\) and \\(w_2\\) are some weights with add up to 1. Basically we are predicting that \\(y_1\\) is a weighted average of the \\(y_{[j]}\\) points for each \\(j \\in \\mathcal{NN}_k(\\mathbf{x}_{1})\\).</p> <p>Specifically, the weights depend upon the distance from \\(\\mathbf{x}_{1}\\) to the corresponding points in the library set.</p> <p>If weight \\(w_i\\) corresponds to library point \\(\\mathbf{x}_{[j]}\\) then we have</p> \\[     w_i \\propto \\exp\\bigl\\{ -\\theta \\, d( \\mathbf{x}_{i} , \\mathbf{x}_{[j]} ) / d_{\\text{min}} \\bigr\\} \\,. \\] <p>Here, the notation \\(d(\\mathbf{x}_i, \\mathbf{x}_{[j]})\\) refers to the distance between prediction point \\(i\\) and library point \\(j\\), and \\(\\theta \\ge 0\\) is a hyperparameter chosen by the user.</p> <p>In summary, we:</p> <ol> <li>Loop through the prediction set, letting \\(i = 1\\) to \\(| \\mathscr{P} |\\):<ol> <li>Start with our covariates \\(\\mathbf{x}_{i}\\) and target \\(y_i\\) from the prediction set, though we temporarily pretend that we don't know \\(y_i\\).</li> <li>Find the \\(k\\) points in the library set that are closest to \\(\\mathbf{x}_{i}\\); in effect, this is constructing a very localised training set of pairs \\(\\mathbf{x}_{[j]} \\to y_{[j]}\\) for \\(j \\in \\mathcal{NN}_k(\\mathbf{x}_{i})\\).</li> <li>Make the prediction \\(\\hat{y}_i\\) as a weighted average of the \\(y_{[j]}\\) neighbour targets.</li> </ol> </li> <li>Summarise the prediction accuracy (e.g. the \\(\\rho\\) correlation) between the \\(\\hat{y}_i\\) predictions and the true \\(y_i\\) values.</li> </ol>"},{"location":"explore/#using-the-s-map-algorithm","title":"Using the S-map algorithm","text":"<p>The alternative choice for <code>algorithm</code> is the S-map procedure, which is short for Sequential Locally Weighted Global Linear Maps.</p> <p>This prediction algorithm uses linear regression to predict each point in the prediction set, though importantly it uses a different linear model for each prediction.</p>"},{"location":"explore/#using-linear-regression","title":"Using linear regression","text":"<p>To make the contrast obvious, let's imagine how traditional linear regression could be used to take some covariates \\(\\mathbf{x}_{i}\\) from \\(\\mathscr{P}\\) and try to predict \\(y_i\\).</p> <p>Linear regression would take the library set and solve the linear system \\(\\overline{\\mathscr{L}} \\boldsymbol{\\beta} = \\mathbf{y}_{\\mathscr{L}}\\) for the \\(\\boldsymbol{\\beta} \\in \\mathbb{R}^{E+1}\\) coefficients given \\(\\overline{\\mathscr{L}} = [\\mathbf{1}; \\mathscr{L}]\\) (similarly \\(\\overline{\\mathbf{x}}_i\\) is \\(\\mathbf{x}_i\\) prepended by a 1).</p> <p>These coeffecients could then be used to make predictions like</p> \\[     \\hat{y}_i = \\overline{\\mathbf{x}}_{i}^\\top \\boldsymbol{\\beta} \\] <p>As the \\(\\overline{\\mathbf{x}}_i\\)'s are short time series trajectories, this method of forecasting is equivalent to fitting an autoregressive process of length \\(E\\).</p>"},{"location":"explore/#back-to-s-map","title":"Back to S-map","text":"<p>The S-map procedure deviates from this process by creating a customised \\(\\boldsymbol{\\beta}_i\\) vector when making each prediction given \\(\\mathbf{x}_i\\) rather than using a shared \\(\\boldsymbol{\\beta}\\) for all of the system.</p> <p>It achieves this by replacing the traditional regression loss function of </p> \\[     \\hat{\\boldsymbol{\\beta}} = \\arg\\min_{\\boldsymbol{\\beta}} | \\mathbf{y}_{\\mathscr{L}} - \\overline{\\mathscr{L}} \\boldsymbol{\\beta} |^2 \\] <p>with a weighted form</p> \\[     \\hat{\\boldsymbol{\\beta}}_i = \\arg\\min_{\\boldsymbol{\\beta}} \\bigl| \\mathbf{w}_i \\circ \\bigl( \\mathbf{y}_{\\mathscr{L}} - \\overline{\\mathscr{L}} \\boldsymbol{\\beta} \\bigr) \\bigr|^2 \\,. \\] <p>The S-map loss function has the extra \\(\\mathbf{w}_i\\) weight vector (included by elementwise multiplication) and the \\(j\\) element of the vector is given by</p> \\[     w_{i,j} = \\exp\\bigl\\{ -\\theta \\, d( \\mathbf{x}_{i} , \\mathbf{x}_{[j]} ) / d_{\\text{mean}} \\bigr\\} \\,. \\] <p>In this equation, \\(d_{\\text{mean}}\\) refers to the average distance between \\(\\mathbf{x}_{i}\\) and every point in \\(\\mathscr{L}\\).</p> <p>Therefore, the S-map method constructs a unique linear regression model when it makes each prediction, and the linear system being constructed is biased so that points closer to the given \\(\\mathbf{x}_{i}\\) covariates are given higher weighting in the regression than those further away. </p> <p>In the extreme case of \\(\\theta = 0\\), then the S-map procedure degenerates to the simple linear regression described above.</p> <p>Normally however we set \\(\\theta &gt; 0\\), and the nearby points heavily influence the S-map predictions and the points very far away from the current \\(\\mathbf{x}_{i}\\) have a negligable impact on the predictions.</p> <p>In fact, since the points very far away from the current point have basically no impact on the S-map predictions, we can simply drop them in order to simplify the computational task. </p> <p>Solving every linear system is a time-consuming process, so only making the linear approximations based on the most relevant \\(k\\) points from the library set can help alleviate the computational burden.</p> <p>To summarise, we:</p> <ol> <li>Loop through the prediction set, letting \\(i = 1\\) to \\(| \\mathscr{P} |\\):<ol> <li>Start with our covariates \\(\\mathbf{x}_{i}\\) and target \\(y_i\\) from the prediction set, though we temporarily pretend that we don't know \\(y_i\\).</li> <li>Find the \\(k\\) points in the library set that are closest to \\(\\mathbf{x}_{i}\\); in effect, this is constructing a very localised training set of pairs \\(\\mathbf{x}_{[j]} \\to y_{[j]}\\) for \\(j \\in \\mathcal{NN}_k(\\mathbf{x}_{i})\\).</li> <li>Solve a weighted linear system to get the coefficients \\(\\boldsymbol{\\beta}_i\\).</li> <li>Make the prediction \\(\\hat{y}_i\\) using the linear combination of \\(\\mathbf{x}_i\\) and \\(\\boldsymbol{\\beta}_i\\).</li> </ol> </li> <li>Summarise the prediction accuracy (e.g. the \\(\\rho\\) correlation) between the \\(\\hat{y}_i\\) predictions and the true \\(y_i\\) values.</li> </ol>"},{"location":"explore/#assessing-the-prediction-quality","title":"Assessing the prediction quality","text":"<p>We calculate the \\(\\hat{y}_i\\) predictions for each target in the prediction set (so \\(i = 1, \\dots, |\\mathscr{P}|\\)), and store the predictions in a vector \\(\\hat{y}\\).</p> <p>As we observe the true value of \\(y_i\\) for most (if not all) of the targets in the prediction set, we can compare our \\(\\hat{y}_i\\) predictions to the observed values. We assess the quality of the predictions using either the correlation</p> \\[ \\rho := \\text{Correlation}(y , \\hat{y}) \\] <p>or using the mean absolute error</p> \\[ \\text{MAE} := \\frac{1}{| \\mathscr{P} |} \\sum_{i=1}^{| \\mathscr{P} |} | y_i - \\hat{y}_i | . \\]"},{"location":"gpu/","title":"GPU acceleration","text":""},{"location":"gpu/#setup","title":"Setup","text":"<p>The <code>edm</code> commands normally run C++ code which run on multiple threads of a CPU, however it is possible to use the <code>gpu</code> option to move the heavy computation to an attached GPU.</p> <p>To use this GPU acceleration, first make sure you are using a Windows or Linux machine which has an NVIDIA graphics card attached (preferably one of either Pascal, Volta, or Turing generations, though others may work if compiled locally). Make sure your graphics drivers are installed (&amp; relatively up-to-date), and install CUDA version 11 or above. Next, install ArrayFire, making sure to select the 'Add to PATH' option. Finally, make sure the latest development version of the <code>edm</code> package is installed by running</p> <pre><code>ssc install edm // If no version of edm has been installed\nedm update, development replace // To get the most up-to-date version\n</code></pre> <p>and your machine should be ready to run the GPU-accelerated.</p>"},{"location":"gpu/#usage","title":"Usage","text":"<p>That means, any <code>edm explore</code> or <code>edm xmap</code> command can use the <code>gpu</code> option and the processing will use the attached GPU.</p> <p>E.g.</p> <pre><code>edm explore x, gpu\nedm xmap x y, gpu\n</code></pre> <p>Alternatively, if you set the global <code>EDM_GPU = 1</code> then all subsequent <code>edm</code> commands will also run on the GPU, e.g.</p> <pre><code>global EDM_GPU = 1\nedm explore x\nedm xmap x y\n</code></pre>"},{"location":"gpu/#contributors","title":"Contributors","text":"<p>The GPU code was contributed by the ArrayFire engineers:</p> <ul> <li>Pradeep Garigipati,</li> <li>Umar Arshad,</li> <li>John Melonakos.</li> </ul> <p>This collaboration was funded by the ARC Discovery Project DP200100219.</p>"},{"location":"missing-data/","title":"Missing data","text":"<p>To explain how the package handles missing data given different options, it is easiest to work by example.</p> <p>Let's say we have the following time series and NA represents a missing value:</p> <p> \\(t\\) \\(a_t\\) 1.0 11 2.5 12 3.0 NA 4.5 14 5.0 15 6.0 16 <p></p> <p>Let's also fix \\(E = 2\\), \\(\\tau = 1\\) and \\(p = 1\\) for these examples.</p> <p>Here we have one obviously missing value for \\(a\\) at time 3. However, there are some hidden missing values also.</p> <p>By default, the package will assume that your data was measured at a regular time interval and will insert missing values as necessary to create a regular grid.</p> <p>For example, the above time series will be treated as if it were sampled every half time unit. So, when creating the \\(E=2\\) manifold it will</p> <p> \\(t\\) \\(a_t\\) 1.0 11 1.5 NA 2.0 NA 2.5 12 3.0 NA 3.5 NA 4.0 NA 4.5 14 5.0 15 5.5 NA 6.0 16 <p></p> <p>The manifold of \\(a\\) and it's projections \\(b\\) will have missing values in them:</p> \\[   M_a = \\left[\\begin{array}{cc}     11 &amp; \\text{NA} \\\\     %\\text{NA} &amp; 11 \\\\     %\\text{NA} &amp; \\text{NA} \\\\     12 &amp; \\text{NA} \\\\     \\text{NA} &amp; 12 \\\\     %\\text{NA} &amp; \\text{NA} \\\\     %\\text{NA} &amp; \\text{NA} \\\\     14 &amp; \\text{NA} \\\\     15 &amp; 14 \\\\     %\\text{NA} &amp; 15 \\\\     16 &amp; \\text{NA} \\\\   \\end{array}\\right]   \\underset{\\small \\text{Matches}}{\\Rightarrow}   \\mathbf{y}_a = \\left[ \\begin{array}{c}     \\text{NA} \\\\     %\\text{NA} \\\\     %12 \\\\     \\text{NA} \\\\     \\text{NA} \\\\     %\\text{NA} \\\\     %14 \\\\     15 \\\\     \\text{NA} \\\\     %16 \\\\     \\text{NA} \\\\   \\end{array} \\right] \\] <p>We can see that the original missing value, combined with some slightly irregular sampling, created a reconstructed manifold that is mostly missing values!</p> <p>By default, the points which contain missing values will not be added to the library or prediction sets.</p> <p>For example, if we let the library and prediction sets be as big as possible then we will have:</p> \\[   \\mathscr{L} = \\emptyset   \\underset{\\small \\text{Matches}}{\\Rightarrow}   \\mathbf{y}_{\\mathscr{L}} = \\emptyset \\] \\[   \\mathscr{P} = \\left[ \\begin{array}{cc}     15 &amp; 14 \\\\   \\end{array} \\right]   \\underset{\\small \\text{Matches}}{\\Rightarrow}   \\mathbf{y}_{\\mathscr{P}} = \\left[ \\begin{array}{c}     \\text{NA} \\\\   \\end{array} \\right] \\] <p>Here we see that the library set is totally empty! This is because for a point to be in the library (with default options) it must be fully observed and the corresponding \\(b\\) projection must also be observed. Similarly, the prediction set is almost empty because (with default options) it must be fully observed.</p>"},{"location":"missing-data/#the-allowmissing-flag","title":"The <code>allowmissing</code> flag","text":"<p>If we set the <code>allowmissing</code> option, then a point is included in the manifold even with some missing values. The only caveats to this rule are:</p> <ul> <li>points which are totally missing will always be discarded,</li> <li>we can't have missing targets for points in the library set.</li> </ul> <p>The largest possible library and prediction sets with <code>allowmissing</code> in this example would be:</p> \\[   \\mathscr{L} = \\left[\\begin{array}{cc}     14 &amp; \\text{NA} \\\\   \\end{array}\\right]   \\underset{\\small \\text{Matches}}{\\Rightarrow}   \\mathbf{y}_{\\mathscr{L}} = \\left[ \\begin{array}{c}     15 \\\\   \\end{array} \\right] \\] \\[   \\mathscr{P} = M_a = \\left[\\begin{array}{cc}     11 &amp; \\text{NA} \\\\     %\\text{NA} &amp; 11 \\\\     %\\text{NA} &amp; \\text{NA} \\\\     12 &amp; \\text{NA} \\\\     \\text{NA} &amp; 12 \\\\     %\\text{NA} &amp; \\text{NA} \\\\     %\\text{NA} &amp; \\text{NA} \\\\     14 &amp; \\text{NA} \\\\     15 &amp; 14 \\\\     %\\text{NA} &amp; 15 \\\\     16 &amp; \\text{NA} \\\\   \\end{array}\\right]   \\underset{\\small \\text{Matches}}{\\Rightarrow}   \\mathbf{y}_{\\mathscr{P}} = y = \\left[ \\begin{array}{c}     \\text{NA} \\\\     %\\text{NA} \\\\     %12 \\\\     \\text{NA} \\\\     \\text{NA} \\\\     %\\text{NA} \\\\     %14 \\\\     15 \\\\     \\text{NA} \\\\     %16 \\\\     \\text{NA} \\\\   \\end{array} \\right] \\] <p>This discussion is implicitly assuming the <code>algorithm</code> is set to the simplex algorithm. When the S-map algorithm is chosen, then we cannot let missing values into the library set \\(\\mathscr{L}\\). This may change in a future implementation of the S-map algorithm.</p>"},{"location":"missing-data/#the-dt-flag","title":"The <code>dt</code> flag","text":"<p>When we add <code>dt</code>, we tell the package to remove missing observations and to also add the time between the observations into the manifold.</p> <p>So, in this example, instead of the observed time series being:</p> <p> \\(t\\) \\(a_t\\) 1.0 11 2.5 12 3.0 NA 4.5 14 5.0 15 6.0 16 <p></p> <p>the <code>dt</code> basically acts as if the supplied data were:</p> <p> \\(t\\) \\(a_t\\) \\(\\mathrm{d}t\\) 1.0 11 1.5 2.5 12 2.0 4.5 14 0.5 5.0 15 1.0 6.0 16 NA <p></p> <p>The resulting manifold and projections are:</p> \\[    M_a = \\left[\\begin{array}{cccc}     12 &amp; 11 &amp; 2.0 &amp; 1.5 \\\\     14 &amp; 12 &amp; 0.5 &amp; 2.0 \\\\     15 &amp; 14 &amp; 1.0 &amp; 0.5 \\\\   \\end{array}\\right]   \\underset{\\small \\text{Matches}}{\\Rightarrow}   \\mathbf{y}_a = \\left[ \\begin{array}{c}     14 \\\\     15 \\\\     16 \\\\   \\end{array} \\right] \\] <p>The largest possible library and prediction sets with <code>dt</code> in this example would be:</p> \\[    \\mathscr{L} = \\mathscr{P} = M_a = \\left[\\begin{array}{cccc}     12 &amp; 11 &amp; 2.0 &amp; 1.5 \\\\     14 &amp; 12 &amp; 0.5 &amp; 2.0 \\\\     15 &amp; 14 &amp; 1.0 &amp; 0.5 \\\\   \\end{array}\\right]   \\underset{\\small \\text{Matches}}{\\Rightarrow}   \\mathbf{y}_{\\mathscr{L}} = \\mathbf{y}_{\\mathscr{P}} = \\mathbf{y}_a = \\left[ \\begin{array}{c}     14 \\\\     15 \\\\     16 \\\\   \\end{array} \\right] \\]"},{"location":"missing-data/#both-allowmissing-and-dt-flags","title":"Both <code>allowmissing</code> and <code>dt</code> flags","text":"<p>If we set both flags, we tell the package to allow missing observations and to also add the time between the observations into the manifold.</p> <p>So our original time series</p> <p> \\(t\\) \\(a_t\\) 1.0 11 2.5 12 3.0 NA 4.5 14 5.0 15 6.0 16 <p></p> <p>will generate the manifold</p> \\[   M_a = \\left[\\begin{array}{cccc}     11 &amp; \\text{NA} &amp; 1.5 &amp; \\text{NA} \\\\     12 &amp; 11 &amp; 0.5 &amp; 1.5 \\\\     \\text{NA} &amp; 12 &amp; 1.5 &amp; 0.5 \\\\     14 &amp; \\text{NA} &amp; 0.5 &amp; 1.5 \\\\     15 &amp; 14 &amp; 1.0 &amp; 0.5 \\\\     16 &amp; 15 &amp; \\text{NA} &amp; 1.0 \\\\   \\end{array}\\right]   \\underset{\\small \\text{Matches}}{\\Rightarrow}   \\mathbf{y}_a = \\left[ \\begin{array}{c}     12 \\\\     \\text{NA} \\\\     14 \\\\     15 \\\\     16 \\\\     \\text{NA}   \\end{array} \\right] \\] <p>and the largest possible library and prediction sets would be</p> \\[   \\mathscr{L} = \\left[\\begin{array}{cccc}     11 &amp; \\text{NA} &amp; 1.5 &amp; \\text{NA} \\\\     \\text{NA} &amp; 12 &amp; 1.5 &amp; 0.5 \\\\     14 &amp; \\text{NA} &amp; 0.5 &amp; 1.5 \\\\     15 &amp; 14 &amp; 1.0 &amp; 0.5 \\\\   \\end{array}\\right]   \\underset{\\small \\text{Matches}}{\\Rightarrow}   \\mathbf{y}_{\\mathscr{L}} = \\left[ \\begin{array}{c}     12 \\\\     14 \\\\     15 \\\\     16 \\\\   \\end{array} \\right] \\] \\[   \\mathscr{P} = M_a = \\left[\\begin{array}{cccc}     11 &amp; \\text{NA} &amp; 1.5 &amp; \\text{NA} \\\\     12 &amp; 11 &amp; 0.5 &amp; 1.5 \\\\     \\text{NA} &amp; 12 &amp; 1.5 &amp; 0.5 \\\\     14 &amp; \\text{NA} &amp; 0.5 &amp; 1.5 \\\\     15 &amp; 14 &amp; 1.0 &amp; 0.5 \\\\     16 &amp; 15 &amp; \\text{NA} &amp; 1.0 \\\\   \\end{array}\\right]   \\underset{\\small \\text{Matches}}{\\Rightarrow}   \\mathbf{y}_{\\mathscr{P}} = \\mathbf{y}_a = \\left[ \\begin{array}{c}     12 \\\\     \\text{NA} \\\\     14 \\\\     15 \\\\     16 \\\\     \\text{NA}   \\end{array} \\right] \\]"},{"location":"time-delayed-embedding/","title":"Time-delayed embedding","text":"<p>The fundamental logic of empirical dynamic modelling are based on Taken's theorem, and the core idea of Taken's theorem is to create these things called time-delayed embeddings (or time-delayed reconstructions). Essentially, we take a time series and break it into many overlapping short trajectories of a fixed length. This page steps through a basic example of this process, highlighting some relevant hyperparameters which can be set.</p> <p>Imagine that we observe a time series \\(a\\).</p> <p>Choose the number of observations</p> <p></p> <p>In tabular form, the data looks like:</p> <p></p> <p>So each one of \\(a_i\\) is an observation of the \\(a\\) time series.</p> <p>To create a time-delayed embedding based on any of these time series, we first need to choose the size of the embedding \\(E\\).</p> <p>Choose a value for \\(E\\)</p> <p></p> <p>The data may be too finely sampled in time. So we select a \\(\\tau\\) which means we only look at every \\(\\tau\\)th observation for each time series.</p> <p>Choose a value for \\(\\tau\\)</p> <p></p> <p>The time-delayed embedding of the \\(a\\) time series with </p> <p>is the manifold:</p> <p></p> <p>The manifold is a collection of these time-delayed embedding vectors. For short, we just refer to each vector as a point on the manifold. While the manifold notation above is the most accurate (a set of vectors) we will henceforward use the more convenient matrix notation:</p> <p></p> <p>Note that the manifold has \\(E\\) columns, and the number of rows depends on the number of observations in the \\(a\\) time series.</p> <p>Why does this look backwards?</p> <p>You may wonder why we have each point in the manifold going backwards in time when reading left-to-right. This is simply an unfortunate convention in the EDM literature which we adhere to.</p>"},{"location":"xmap/","title":"The <code>xmap</code> subcommand","text":"<p>The <code>explore</code> subcommand focuses on using a the history of a time series to predict itself in the future.</p> <p>On the other hand, the <code>xmap</code> subcommand - which is short for cross mapping - focuses on using one time series to predict the value of a different time series.</p> <p>This process is central to the 'convergent cross mapping' algorithm which we use to decide if one time series has a causal effect on another.</p>"},{"location":"xmap/#setup","title":"Setup","text":"<p>Imagine that we use the command:</p> <pre><code>edm xmap a b, oneway\n</code></pre> <p>This will consider two different time series, here labelled \\(a\\) and \\(b\\).</p> <p>Choose the number of observations</p> <p></p> <p>In tabular form, the data looks like:</p> <p></p> <p>Choose a value for \\(E\\)</p> <p></p> <p>Choose a value for \\(\\tau\\)</p> <p></p> <p>The lagged embedding \\(M_a\\) is constructed:</p> <p></p>"},{"location":"xmap/#library-and-prediction-sets","title":"Library and prediction sets","text":"<p>In <code>xmap</code> mode, the library set \\(\\mathscr{L}\\) is typically the first \\(L\\) points of \\(M_a\\). The library size parameter \\(L\\) is set by the Stata parameter <code>library</code>.</p> <p>Choose a value for \\(L\\)</p> <p></p> <p></p> <p>However, in <code>xmap</code> mode the prediction set \\(\\mathscr{P}\\) will include every point of the \\(a\\) embedding so:</p> <p></p>"},{"location":"xmap/#targets-come-from-the-other-time-series","title":"Targets come from the other time series","text":"<p>The \\(b\\) time series will be the values which we try to predict. Here we are trying to predict \\(p\\) observations ahead, where the default case is actually \\(p = 0\\). The \\(p = 0\\) case means we are using the \\(a\\) time series to try to predict the contemporaneous value of \\(b\\). A negative \\(p\\) may be chosen, though this is a bit abnormal.</p> <p>Choose a value for \\(p\\)</p> <p></p> <p></p> <p></p>"},{"location":"xmap/#what-does-edm-xmap-a-b-do","title":"What does <code>edm xmap a b</code> do?","text":"<p>In <code>xmap</code>, we perform a series of predictions in very similar manner to the <code>explore</code> subcommand.  The difference is our library and prediction sets contain values from the \\(a\\) time series whereas the \\(\\mathbf{y}_{\\mathscr{L}}\\) and \\(\\mathbf{y}_{\\mathscr{P}}\\) vectors contain (usually contemporaneous) values from the \\(b\\) time series.</p> \\[     \\begin{aligned}         \\text{Given } \\underbrace{ \\mathbf{x}_{i} }_{\\text{From } a} \\text{ and target }\\underbrace{ y_i }_{\\text{From } b}         &amp;          \\underset{\\small \\text{Find neighbours in } \\mathscr{L} }{\\Rightarrow}         ( \\underbrace{ \\mathbf{x}_{[j]} }_{\\text{From } a} , \\underbrace{ y_{[j]} }_{\\text{From } b} )_{j \\in \\mathcal{NN}_k(i)}         \\underset{\\small \\text{Make prediction}}{\\Rightarrow}         \\underbrace{ \\hat{y}_i }_{\\text{For } b }     \\end{aligned} \\] <p>This means that we are learning the mapping from (the recent history of) the \\(a_t\\) time series to the (contemporaneous value of the) \\(b_t\\) time series.</p>"},{"location":"xmap/#convergent-cross-mapping","title":"Convergent cross mapping","text":"<p>Typically the goal of using <code>xmap</code> is to specify a grid of values for the <code>library</code> size and look at the relationship between the library size \\(L\\) and the predictive performance \\(\\rho\\).</p> <p>If the <code>xmap a b</code> predictive performance increases significantly when more data to learn from (i.e. as \\(L\\) increases) then the convergent cross mapping (CCM) algorithm says that this is evidence that the time series \\(b_t\\) has a causal effect on \\(a_t\\).</p> <p>This may seem backwards at first glance, but the logic is that if \\(b_t\\) has a causal effect on \\(a_t\\), then information about \\(b_t\\) is embedded in the time series \\(a_t\\).</p> <p>Hence, if we get more and more data from \\(a_t\\), we can glean more information about the cause \\(b_t\\) and hence make better prediction of \\(b_t\\) as \\(L\\) increases.</p> <p>The summary which the <code>edm</code> command prints out uses the notation <code>b|M(a)</code> to indicate that we use the reconstructed manifold of \\(a_t\\) to make predictions about \\(b_t\\), and so the causal effect of \\(b\\) to \\(a\\) can be read left-to-right.</p> <p>See the examples/vignettes to see the CCM process in action.</p>"},{"location":"examples/chicago/","title":"Chicago crime/temperature example","text":"<p>To demonstrate the usefulness of EDM in estimating the impact of causal variables, we use a real-world dataset that reflects daily temperature and crime levels in Chicago, which we make available in the chicago.dta file.</p>"},{"location":"examples/chicago/#the-data","title":"The data","text":"<p>First, we load the time series from the <code>chicago.dta</code> file:</p> <pre><code>. use chicago, clear\n</code></pre> <p>Plotting the data gives:</p> <p><pre><code>. scatter crime temp\n</code></pre> </p> <p>A linear correlation of the dataset... </p> <pre><code>. corr crime temp\n(obs=4,371)\n\n             |    crime     temp\n-------------+------------------\n       crime |   1.0000\n        temp |   0.4620   1.0000\n</code></pre> <p>shows a mild correlation, however the causal structure (if any) and its direction is not shown.</p>"},{"location":"examples/chicago/#find-the-optimal-embedding-dimension","title":"Find the optimal embedding dimension","text":"<p>Now we use <code>edm explore</code> to find the optimal embedding dimension of the \\(\\texttt{Temperature}\\) time series. We check the values of \\(E = 2, \\dots 20\\). The <code>crossfold(5)</code> option means that, for each \\(E\\) value we run 5 sets of predictions, and for each set we take four fifths of the data for training and predict the remaining one fifth.</p> <pre><code>. edm explore temp, e(2/20) crossfold(5)\n5-fold cross-validation progress (5 in total)\nPercent complete: 0...10...20...30...40...50...60...70...80...90...\n\nEmpirical Dynamic Modelling\nUnivariate mapping with temp and its lag values\n----------------------------------------------------------------------\n                      --------- rho ---------  --------- MAE ---------\n Actual E    theta         Mean    Std. Dev.         Mean    Std. Dev.\n----------------------------------------------------------------------\n        2        1       .89818      .010487       7.1032       .20784\n        3        1       .90439     .0080387       6.9064       .27909\n        4        1       .91028     .0095908       6.6776       .29265\n        5        1       .91548      .010502       6.4794       .37096\n        6        1       .91879     .0078932       6.4002       .30358\n        7        1       .91934     .0071129       6.3696        .2972\n        8        1       .91798     .0070104       6.4607       .25364\n        9        1       .91722     .0071189       6.5027       .27352\n       10        1       .91762     .0073315       6.5053       .28581\n       11        1       .91561     .0077983       6.5573       .31192\n       12        1       .91638      .007973       6.5689       .32428\n       13        1       .91524     .0083123       6.6081       .31755\n       14        1       .91485     .0086085       6.6257       .35135\n       15        1        .9133     .0083516       6.6861       .32796\n       16        1       .91289     .0092333       6.7024       .35415\n       17        1       .91183     .0094544       6.7338       .35987\n       18        1       .91029     .0089739       6.7972       .35502\n       19        1       .90853      .008125       6.8699       .33429\n       20        1       .90766     .0091983       6.9047       .31412\n----------------------------------------------------------------------\nNote: Results from 5 runs\nNote: Number of neighbours (k) is set to between 3 and 21\nNote: 5-fold cross validation results reported\n</code></pre> <p>From the <code>rho</code> column we can see that the prediction accuracy is maximised when \\(E = 7\\), so we take this as our estimate of the embedding dimension.</p>"},{"location":"examples/chicago/#convergent-cross-mapping","title":"Convergent cross-mapping","text":"<p>The <code>edm xmap</code> command will run the cross-mapping task, which allows us to ascertain the causal links between the crime and temperature time series.</p> <pre><code>. qui edm xmap temp crime, e(7) rep(4) ///\n&gt;         library(10(5)200 210(10)1000 1020(20)2000 2050(50)4350 4365) \n</code></pre> <p>Note</p> <pre><code>This selects a lot of library points, and replicates the analysis some times, so this command may take a minute or two to finish.\nChoosing a machine with more CPU cores or faster cores will help significantly.\n</code></pre> <p>Plotting the results gives:</p> <p><pre><code>. mat cyx = e(xmap_2)\n\n. mat cxy = e(xmap_1)\n\n. svmat cyx, names(chicago_yx)\n\n. svmat cxy, names(chicago_xy)\n\n. label variable chicago_xy3 \"Crime|M(Temperature)\"\n\n. label variable chicago_yx3 \"Temperature|M(Crime)\"\n\n. twoway (scatter chicago_xy3 chicago_xy2, mfcolor(%30) mlcolor(%30)) ///\n&gt;     (scatter chicago_yx3 chicago_yx2, mfcolor(%30) mlcolor(%30)) ///\n&gt;     (lpoly chicago_xy3 chicago_xy2)(lpoly chicago_yx3 chicago_yx2), ///\n&gt;         xtitle(L) ytitle(\"{it:{&amp;rho}}\") legend(col(1))\n\n. drop chicago_xy* chicago_yx*\n</code></pre> </p> <p>In this plot, we can see that one direction shows a significant increase in accuracy as \\(L\\) increases, whereas the other direction is pretty flat. The direction which increases the most is the \\(\\texttt{Temperature} \\mid M(\\texttt{Crime})\\) direction. This notation means we used  \\(\\texttt{Crime}\\) to predict \\(\\texttt{Temperature}\\), and due to the backward nature of EDM means it refers to the causal link \\(\\texttt{Temperature} \\to M(\\texttt{Crime})\\). Therefore, we'd conclude that there is a causal link from temperature to crime, though no link in the reverse direction (which would be implausible).  </p>"},{"location":"examples/chicago/#inspecting-the-s-map-coefficients","title":"Inspecting the S-map coefficients","text":"<p>If we run <code>xmap</code> with the <code>savesmap(beta)</code> option, we can store the fitted S-map coefficients into variable which start with the prefix <code>beta</code>.</p> <pre><code>. edm xmap temp crime, e(7) alg(smap) k(-1) savesmap(beta)\nPercent complete: 0...10...20...30...40...50...60...70...80...90...\nPercent complete: 0...10...20...30...40...50...60...70...80...90...\n\nEmpirical Dynamic Modelling\nConvergent Cross-mapping result for variables temp and crime\n--------------------------------------------------------------------------\n                 Mapping    Library size             rho             MAE\n--------------------------------------------------------------------------\n   crime ~ crime|M(temp)            4365          .46784          136.93 \n    temp ~ temp|M(crime)            4365          .54886          14.661 \n--------------------------------------------------------------------------\nNote: Number of neighbours (k) is set to 4364\nNote: The embedding dimension E is 7\n</code></pre> <p>For example, the coefficient variables that are created are:</p> <pre><code>. ds beta*, detail\n\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------------------------------------------------------------------------\nbeta1_b0_rep1   double  %10.0g                constant in temp predicting crime S-map equation (rep 1)\nbeta1_b1_rep1   double  %10.0g                temp predicting crime or crime|M(temp) S-map coefficient (rep 1)\nbeta1_b2_rep1   double  %10.0g                l1.temp predicting crime or crime|M(temp) S-map coefficient (rep 1)\nbeta1_b3_rep1   double  %10.0g                l2.temp predicting crime or crime|M(temp) S-map coefficient (rep 1)\nbeta1_b4_rep1   double  %10.0g                l3.temp predicting crime or crime|M(temp) S-map coefficient (rep 1)\nbeta1_b5_rep1   double  %10.0g                l4.temp predicting crime or crime|M(temp) S-map coefficient (rep 1)\nbeta1_b6_rep1   double  %10.0g                l5.temp predicting crime or crime|M(temp) S-map coefficient (rep 1)\nbeta1_b7_rep1   double  %10.0g                l6.temp predicting crime or crime|M(temp) S-map coefficient (rep 1)\nbeta2_b0_rep1   double  %10.0g                constant in crime predicting temp S-map equation (rep 1)\nbeta2_b1_rep1   double  %10.0g                crime predicting temp or temp|M(crime) S-map coefficient (rep 1)\nbeta2_b2_rep1   double  %10.0g                l1.crime predicting temp or temp|M(crime) S-map coefficient (rep 1)\nbeta2_b3_rep1   double  %10.0g                l2.crime predicting temp or temp|M(crime) S-map coefficient (rep 1)\nbeta2_b4_rep1   double  %10.0g                l3.crime predicting temp or temp|M(crime) S-map coefficient (rep 1)\nbeta2_b5_rep1   double  %10.0g                l4.crime predicting temp or temp|M(crime) S-map coefficient (rep 1)\nbeta2_b6_rep1   double  %10.0g                l5.crime predicting temp or temp|M(crime) S-map coefficient (rep 1)\nbeta2_b7_rep1   double  %10.0g                l6.crime predicting temp or temp|M(crime) S-map coefficient (rep 1)\n</code></pre> <p>Plotting them allows us to see the contemporaneous effect of temperature on crime.</p> <p><pre><code>. twoway (kdensity beta1_b1_rep1), ytitle(\"Density\") ///\n&gt;         xtitle(\"Contemporaneous effect of temperature on crime\")\n</code></pre> </p> <p><pre><code>. twoway (scatter beta1_b1_rep1 temp, xtitle(\"Temperature (Fahrenheit)\") ///\n&gt;         ytitle(\"Contemporaneous effect of temperature on crime\") ///\n&gt;         msize(small))(lpoly beta1_b1_rep1 temp), ///\n&gt;         legend(on order( 1 \"Local coefficient\" 2 \"Local polynomial smoothing\"))\n</code></pre> </p>"},{"location":"examples/logistic-map/","title":"Logistic map","text":""},{"location":"examples/logistic-map/#setting-up-the-synthetic-data","title":"Setting up the synthetic data","text":"<p>First, we tell Stata that we're working with time series using <code>tsset</code>.</p> <pre><code>. set obs 500\nNumber of observations (_N) was 0, now 500.\n\n. \n. gen t = _n\n\n. tsset t\n\nTime variable: t, 1 to 500\n        Delta: 1 unit\n</code></pre> <p>This is important, as the <code>edm</code> relies on the user to specify which variable corresponds to 'time'.</p> <p>Next, we can generate the synthetic data which we will use in the causal analysis.</p> <pre><code>. gen x = 0.2 if _n==1\n(499 missing values generated)\n\n. gen y = 0.3 if _n==1\n(499 missing values generated)\n\n. gen z = 0.1 if _n==1\n(499 missing values generated)\n\n. \n. local r_x 3.79\n\n. local r_y 3.79\n\n. local r_z 3.77\n\n. local beta_xy = 0.0\n\n. local beta_yx=0.2\n\n. local beta_zy = 0.5\n\n. local tau = 1\n\n. drawnorm u1 u2\n\n. \n. forvalues i=2/`=_N' {\n  2.     qui replace x=l.x *(`r_x' *(1-l.x)-`beta_xy'*l.y) in `i'\n  3.     qui replace y=l.y *(`r_y' *(1-l.y)-`beta_yx'*l`tau'.x) in `i'\n  4.     qui replace z=l.z *(`r_z' *(1-l.z)-`beta_zy'*l`tau'.y) in `i'\n  5. }\n\n. keep in 300/450\n(349 observations deleted)\n</code></pre> <p>Now we have two time series, \\(x\\) and \\(y\\). For example, the first ten joint observations look like: </p> <pre><code>. list x y if _n &lt;= 10\n\n     +---------------------+\n     |        x          y |\n     |---------------------|\n  1. | .5353563   .3485506 |\n  2. | .9427623   .8232493 |\n  3. | .2045144   .3962567 |\n  4. | .6165885   .8905014 |\n  5. |  .895983   .2597431 |\n     |---------------------|\n  6. | .3532184   .6821833 |\n  7. | .8658451    .773515 |\n  8. | .4402364   .5300195 |\n  9. | .9339633   .8974178 |\n 10. | .2337515    .181273 |\n     +---------------------+\n</code></pre> <p>Plotting the two time series together looks like:</p> <p></p>"},{"location":"examples/logistic-map/#find-the-optimal-embedding-dimension","title":"Find the optimal embedding dimension","text":"<p>Now we use <code>edm explore</code> to find the optimal embedding dimension of the \\(y\\) time series. We check the values of \\(E = 2, \\dots 10\\), and use <code>rep(50)</code> to take 50 random subsets of the data to use for training (leaving the other random half for prediction). </p> <pre><code>. edm explore y, e(2/10) rep(50)\nReplication progress (50 in total)\nPercent complete: 0...10...20...30...40...50...60...70...80...90...\n\nEmpirical Dynamic Modelling\nUnivariate mapping with y and its lag values\n----------------------------------------------------------------------\n                      --------- rho ---------  --------- MAE ---------\n Actual E    theta         Mean    Std. Dev.         Mean    Std. Dev.\n----------------------------------------------------------------------\n        2        1       .98104     .0056417       .03127     .0044271\n        3        1       .96996      .012497        .0393     .0054778\n        4        1       .95131      .016597      .049013     .0060371\n        5        1       .92509       .02697      .059667     .0082334\n        6        1       .89149      .039457       .07092     .0097508\n        7        1       .83573       .04969      .085179      .010616\n        8        1       .78633      .058107      .098735      .011921\n        9        1       .74695      .066014       .10839      .012128\n       10        1       .71795      .071137       .11598      .011756\n----------------------------------------------------------------------\nNote: Results from 50 runs\nNote: Number of neighbours (k) is set to between 3 and 11\nNote: Random 50/50 split for training and validation data\n\n. mat r= e(explore_result)\n\n. svmat r, names(col)\nnumber of observations will be reset to 450\nPress any key to continue, or Break to abort\nNumber of observations (_N) was 151, now 450.\n\n. twoway (scatter c3 c1)(lpoly c3 c1),xtitle(\"E\") ytitle(\"{it:{&amp;rho}}\") ///\n&gt;         legend(order(1 \"{it:{&amp;rho}}\" 2 \"local polynomial smoothing\") ///\n&gt;         col(1) position(8) ring(0))\n\n. drop c*\n</code></pre> <p>From the <code>rho</code> column we can see the prediction accuracy decreasing as \\(E\\) increases, so \\(E=2\\) is our best choice for the embedding dimension. Plotting the same results:</p> <p><pre><code>. mat r= e(explore_result)\n\n. svmat r, names(col)\n\n. twoway (scatter c3 c1)(lpoly c3 c1),xtitle(\"E\") ytitle(\"{it:{&amp;rho}}\") ///\n&gt;         legend(order(1 \"{it:{&amp;rho}}\" 2 \"local polynomial smoothing\") ///\n&gt;         col(1) position(8) ring(0))\n\n. drop c*\n</code></pre> </p>"},{"location":"examples/logistic-map/#assess-the-level-of-non-linearity-in-the-time-series","title":"Assess the level of non-linearity in the time series","text":"<p>Another use of <code>edm explore</code> is to check if the observed time series exhibits high levels of non-linearity. Here, we use the S-map algorithm and vary \\(\\theta = 0, \\dots, 5\\), using all of the training set as neighbours (i.e. <code>k(-1)</code>).</p> <pre><code>. edm explore y, e(2) algorithm(smap) theta(0(1)5) k(-1) \nPercent complete: 0...10...20...30...40...50...60...70...80...90...\n\nEmpirical Dynamic Modelling\nUnivariate mapping with y and its lag values\n--------------------------------------------------------------------\n          Actual E           theta             rho             MAE\n--------------------------------------------------------------------\n                 2               0           .7276          .12482 \n                 2               1           .9594         .057205 \n                 2               2          .98107         .035173 \n                 2               3          .98825         .027744 \n                 2               4          .99162         .023781 \n                 2               5            .993         .021781 \n--------------------------------------------------------------------\nNote: Number of neighbours (k) is set to 75\nNote: 50/50 split for training and validation data\n</code></pre> <p>Showing the same <code>rho</code>/\\(\\rho\\) prediction accuracies as a plot:</p> <p><pre><code>. mat r = e(explore_result)\n\n. svmat r, names(col)\nnumber of observations will be reset to 501\nPress any key to continue, or Break to abort\nNumber of observations (_N) was 450, now 501.\n\n. twoway (line c3 c2) , legend(order(1 \"{it:{&amp;rho}}\") position(5) ring(0)) ///\n&gt;         xtitle(\"{it:{&amp;theta}}\") ytitle(\"{it:{&amp;rho}}\") ///\n&gt;         title(\"{it:{&amp;rho}}-{it:{&amp;theta}} of variable y\")\n\n. drop c*\n</code></pre> </p> <p>As the accuracy climbs as larger for \\(\\theta &gt; 0\\) compared to \\(\\theta = 0\\), we deduce that the time series is likely the output of a non-linear system. This is important, as the theory underlying EDM is specific to non-linear systems.</p>"},{"location":"examples/logistic-map/#convergent-cross-mapping","title":"Convergent cross-mapping","text":"<p>Finally, now we are satisfied that the time series are non-linear and we have selected \\(E=2\\) as the embedding dimension, we can run convergent cross-mappping. If the prediction accuracy increases as the library size \\(L\\) increases, then we can say this is evidence of a causal link in that direction. </p> <pre><code>. qui edm xmap x y, e(2) rep(10) library(5/150)\n</code></pre> <p>Using these predictions, we can plot the accuracy against the library size: </p> <p><pre><code>. mat c1 = e(xmap_1)\n\n. mat c2 = e(xmap_2)\n\n. svmat c1, names(xy)\nnumber of observations will be reset to 1460\nPress any key to continue, or Break to abort\nNumber of observations (_N) was 501, now 1,460.\n\n. svmat c2, names(yx)\n\n. label variable xy3 \"y|M(x)\"\n\n. label variable yx3 \"x|M(y)\"\n\n. twoway (scatter xy3 xy2, mfcolor(%30) mlcolor(%30)) ///\n&gt;     (scatter yx3 yx2, mfcolor(%30) mlcolor(%30)) ///\n&gt;     (lpoly xy3 xy2)(lpoly yx3 yx2), xtitle(L) ytitle(\"{it:{&amp;rho}}\") ///\n&gt;         legend(col(2))\n</code></pre> </p> <p>As both plots of the accuracy are significantly increasing as \\(L\\) increases, then we can say there is evidence of both \\(x \\to y\\) and \\(y \\to x\\) causal links. The direction which increases the most is the \\(x \\mid M(y)\\) direction. This notation means we used \\(y\\) to predict \\(x\\), and due to the backward nature of EDM means it refers to the causal link \\(x \\to y\\). Therefore, we'd conclude those both directions show causality, though the \\(x \\to y\\) link is stronger in the data.  </p>"}]}